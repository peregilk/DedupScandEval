{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"mcc": 0.3180043641453269, "macro_f1": 0.40631675269958095}, {"mcc": 0.32772922561766626, "macro_f1": 0.3740459034462384}, {"mcc": 0.35931027934129744, "macro_f1": 0.4011654977613211}, {"mcc": 0.3698013702197799, "macro_f1": 0.4007573324769857}, {"mcc": 0.34216457716087934, "macro_f1": 0.38505877524806364}, {"mcc": 0.3261065179285312, "macro_f1": 0.3967095817617891}, {"mcc": 0.32155243082298524, "macro_f1": 0.4000126190786644}, {"mcc": 0.3390972520778914, "macro_f1": 0.4040768980148224}, {"mcc": 0.33027603927359117, "macro_f1": 0.3980074889649357}, {"mcc": 0.3575827846558921, "macro_f1": 0.41655295672725307}]}, "total": {"test_mcc": 33.91624841243841, "test_mcc_se": 1.0992565553624338, "test_macro_f1": 39.82703806179654, "test_macro_f1_se": 0.7199103036525796}}, "num_model_parameters": 7248023552, "max_sequence_length": 2048, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0059271803556308214, "micro_f1": 0.010241404535479153}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.01405152224824356, "micro_f1": 0.013005780346820808}, {"micro_f1_no_misc": 0.012981393336218087, "micro_f1": 0.013888888888888886}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}]}, "total": {"test_micro_f1_no_misc": 0.3296009594009247, "test_micro_f1_no_misc_se": 0.35331776480401683, "test_micro_f1": 0.37136073771188843, "test_micro_f1_se": 0.37475892869943356}}, "num_model_parameters": 7248023552, "max_sequence_length": 2173, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"mcc": 0.004830743314301972, "macro_f1": 0.44381076530335123}, {"mcc": -0.051793725693266714, "macro_f1": 0.4732666123966571}, {"mcc": 0.0509865882797073, "macro_f1": 0.5248967933518587}, {"mcc": 0.023008032903655173, "macro_f1": 0.5113790467501458}, {"mcc": 0.019679091625121617, "macro_f1": 0.5095593601969212}, {"mcc": 0.003246426586498755, "macro_f1": 0.501192004401247}, {"mcc": 0.07447828661663891, "macro_f1": 0.5341645856444647}, {"mcc": -0.04103232102626644, "macro_f1": 0.4160053543429596}, {"mcc": 0.02827013066848604, "macro_f1": 0.4141637979506939}, {"mcc": 0.023860254366531473, "macro_f1": 0.49437492401317823}]}, "total": {"test_mcc": 1.3553350764140808, "test_mcc_se": 2.355171899772565, "test_macro_f1": 48.228132443514774, "test_macro_f1_se": 2.7092733078591418}}, "num_model_parameters": 7248023552, "max_sequence_length": 2048, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "norquad", "task": "question-answering", "dataset_languages": ["nb", "nn", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"em": 16.377171215880892, "f1": 40.808163169389175}, {"em": 10.693400167084377, "f1": 28.27777260461522}, {"em": 11.278195488721805, "f1": 40.74202432703007}, {"em": 8.850289495450786, "f1": 30.81649192363628}, {"em": 17.255546425636812, "f1": 38.21808489773192}, {"em": 14.108910891089108, "f1": 37.09203384918843}, {"em": 11.70651277823578, "f1": 35.11982080943633}, {"em": 6.827643630308077, "f1": 34.43670022280005}, {"em": 12.781954887218046, "f1": 32.45238291206605}, {"em": 3.4338358458961475, "f1": 16.02816880166706}]}, "total": {"test_em": 11.331346082552184, "test_em_se": 2.611175042995842, "test_f1": 33.39916435175606, "test_f1_se": 4.551131477742573}}, "num_model_parameters": 7248023552, "max_sequence_length": 2077, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"bertscore": 0.5022184011249919, "rouge_l": 0.06827861495974583}, {"bertscore": 0.48727020828664536, "rouge_l": 0.057550291200198}, {"bertscore": 0.5329418246328714, "rouge_l": 0.06685192092796408}, {"bertscore": 0.4827645829936955, "rouge_l": 0.055654239611809625}, {"bertscore": 0.5059198773742537, "rouge_l": 0.061928766586353295}, {"bertscore": 0.5196152541757328, "rouge_l": 0.06466573176167506}, {"bertscore": 0.4849950706557138, "rouge_l": 0.06788705691680867}, {"bertscore": 0.45576899428124307, "rouge_l": 0.05441990706944179}, {"bertscore": 0.50930146612518, "rouge_l": 0.06306633469122802}, {"bertscore": 0.547178532127873, "rouge_l": 0.07613828951809501}]}, "total": {"test_bertscore": 50.279742117782014, "test_bertscore_se": 1.650827476043819, "test_rouge_l": 6.364411532433192, "test_rouge_l_se": 0.41126506772091026}}, "num_model_parameters": 7248023552, "max_sequence_length": 2301, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"mcc": 0.01902806087179168, "accuracy": 0.25}, {"mcc": -0.0060410395280495275, "accuracy": 0.24169921875}, {"mcc": 0.022181693237602818, "accuracy": 0.248046875}, {"mcc": 0.04242013277338083, "accuracy": 0.27294921875}, {"mcc": 0.0235176839368962, "accuracy": 0.26220703125}, {"mcc": -0.0009295840399968483, "accuracy": 0.24755859375}, {"mcc": 0.018233836273777296, "accuracy": 0.24853515625}, {"mcc": -0.007453568580806295, "accuracy": 0.23779296875}, {"mcc": 0.009701904081643427, "accuracy": 0.24609375}, {"mcc": -0.019975445899632984, "accuracy": 0.23681640625}]}, "total": {"test_mcc": 1.006836731266066, "test_mcc_se": 1.1524354937482884, "test_accuracy": 24.9169921875, "test_accuracy_se": 0.6813766781680083}}, "num_model_parameters": 7248023552, "max_sequence_length": 2048, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"accuracy": 0.2529296875, "mcc": 0.009463669501699577}, {"accuracy": 0.2412109375, "mcc": -0.01237273930653688}, {"accuracy": 0.26171875, "mcc": -0.002990044676634657}, {"accuracy": 0.2490234375, "mcc": -0.01263138989331391}, {"accuracy": 0.248046875, "mcc": -0.0007058787400517994}, {"accuracy": 0.23876953125, "mcc": 0.007414890179614953}, {"accuracy": 0.26123046875, "mcc": 0.011435622063793243}, {"accuracy": 0.2578125, "mcc": -0.0016205568891454226}, {"accuracy": 0.23583984375, "mcc": -0.023052358954543505}, {"accuracy": 0.2255859375, "mcc": -0.021768091257180773}]}, "total": {"test_accuracy": 24.7216796875, "test_accuracy_se": 0.733453443112214, "test_mcc": -0.4682687797229917, "test_mcc_se": 0.7691601002248549}}, "num_model_parameters": 7248023552, "max_sequence_length": 2048, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "NbAiLab/mimir-mistral-7b-books", "results": {"raw": {"test": [{"test_speed": 1350.56, "test_speed_short": 166.98}, {"test_speed": 2344.1600000000003, "test_speed_short": 288.2}, {"test_speed": 2918.56, "test_speed_short": 570.76}, {"test_speed": 3721.3599999999997, "test_speed_short": 667.4}, {"test_speed": 3882.68, "test_speed_short": 788.48}, {"test_speed": 4173.400000000001, "test_speed_short": 1077.44}, {"test_speed": 4442.96, "test_speed_short": 1222.5900000000001}, {"test_speed": 4866.28, "test_speed_short": 1364.36}, {"test_speed": 4839.5199999999995, "test_speed_short": 1492.78}, {"test_speed": 4843.74, "test_speed_short": 1626.8999999999999}]}, "total": {"test_speed": 3738.322, "test_speed_se": 737.1648490426, "test_speed_short": 926.5889999999999, "test_speed_short_se": 314.0377999273304}}, "num_model_parameters": 7248023552, "max_sequence_length": 2046, "vocabulary_size": 32768, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
