{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"mcc": 0.5335524245372144, "macro_f1": 0.6958221904236458}, {"mcc": 0.44509843557645157, "macro_f1": 0.6355004232304845}, {"mcc": 0.4749638384795294, "macro_f1": 0.648153878025446}, {"mcc": 0.5415534450570629, "macro_f1": 0.6975734896836219}, {"mcc": 0.3848655414900729, "macro_f1": 0.5803136528942981}, {"mcc": 0.5014216217140637, "macro_f1": 0.6772475355026065}, {"mcc": 0.35419846540512734, "macro_f1": 0.5099278716065802}, {"mcc": 0.5032429534116442, "macro_f1": 0.667154109548318}, {"mcc": 0.4901498058356064, "macro_f1": 0.6704292640362435}, {"mcc": 0.5115666204295525, "macro_f1": 0.6783326321373128}]}, "total": {"test_mcc": 47.40613151936326, "test_mcc_se": 3.838725217045576, "test_macro_f1": 64.60455047088557, "test_macro_f1_se": 3.6465920583791096}}, "num_model_parameters": 7241732096, "max_sequence_length": 32768, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.0008928571428571428, "micro_f1": 0.0007849293563579278}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.001594896331738437, "micro_f1": 0.0014326647564469914}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0024803637866887144, "micro_f1": 0.0029347028613352895}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.03137902559867878, "micro_f1": 0.035928143712574856}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}]}, "total": {"test_micro_f1_no_misc": 0.3634714285996307, "test_micro_f1_no_misc_se": 0.6065892537887057, "test_micro_f1": 0.4108044068671506, "test_micro_f1_se": 0.6955322883259628}}, "num_model_parameters": 7241732096, "max_sequence_length": 32768, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0007499062617172853, "micro_f1": 0.0006576783952647156}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0007748934521503294, "micro_f1": 0.0006568144499178982}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0022979701263883567, "micro_f1": 0.0013382402141184342}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.026490066225165563, "micro_f1": 0.026299311208515964}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}]}, "total": {"test_micro_f1_no_misc": 0.30312836065421533, "test_micro_f1_no_misc_se": 0.5128896749468068, "test_micro_f1": 0.2895204426781701, "test_micro_f1_se": 0.5104730922207208}}, "num_model_parameters": 7241732096, "max_sequence_length": 32768, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"mcc": 0.06498288551537772, "macro_f1": 0.38015899608977466}, {"mcc": 0.11190712610046391, "macro_f1": 0.35988535496105406}, {"mcc": 0.16502965103223843, "macro_f1": 0.4809349479019994}, {"mcc": 0.020929437928120163, "macro_f1": 0.34690884772472497}, {"mcc": 0.18223055650680425, "macro_f1": 0.4747653313947389}, {"mcc": 0.068429806142434, "macro_f1": 0.3577209713323956}, {"mcc": 0.04244978199745053, "macro_f1": 0.3505885823205678}, {"mcc": -0.022733267016429453, "macro_f1": 0.3394009816433955}, {"mcc": 0.0601766213343753, "macro_f1": 0.36053429677808857}, {"mcc": 0.1733804484205868, "macro_f1": 0.44984111384111386}]}, "total": {"test_mcc": 8.667830479614217, "test_mcc_se": 4.293097394804877, "test_macro_f1": 39.00739423987854, "test_macro_f1_se": 3.4521417957942004}}, "num_model_parameters": 7241732096, "max_sequence_length": 32643, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"mcc": 0.05303687093867944, "macro_f1": 0.4158863535410913}, {"mcc": 0.08666551659891135, "macro_f1": 0.4043882161151683}, {"mcc": 0.07696669527473103, "macro_f1": 0.3895216318855641}, {"mcc": 0.06629323814266523, "macro_f1": 0.395930095121025}, {"mcc": 0.09435899742866691, "macro_f1": 0.49749506892923057}, {"mcc": 0.11759710666282404, "macro_f1": 0.3787481424879718}, {"mcc": 0.06158599460828678, "macro_f1": 0.3456879206920896}, {"mcc": 0.03380381454327528, "macro_f1": 0.3694484816033045}, {"mcc": 0.07835032388404513, "macro_f1": 0.4175669112551502}, {"mcc": 0.03277534037785612, "macro_f1": 0.36820491429834135}]}, "total": {"test_mcc": 7.014338984599414, "test_mcc_se": 1.6428136950584098, "test_macro_f1": 39.82877735928937, "test_macro_f1_se": 2.5745252036001793}}, "num_model_parameters": 7241732096, "max_sequence_length": 32643, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "norquad", "task": "question-answering", "dataset_languages": ["nb", "nn", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"em": 48.38709677419355, "f1": 72.03419850138776}, {"em": 36.340852130325814, "f1": 60.04413520254047}, {"em": 52.88220551378446, "f1": 74.34397175089298}, {"em": 46.650124069478906, "f1": 72.59273312630125}, {"em": 47.98685291700904, "f1": 71.23540787014008}, {"em": 55.693069306930695, "f1": 76.62755474976052}, {"em": 47.32069249793899, "f1": 72.80204167408512}, {"em": 48.2098251457119, "f1": 71.68787664686909}, {"em": 44.611528822055135, "f1": 71.35213357266227}, {"em": 41.79229480737018, "f1": 66.61372421324918}]}, "total": {"test_em": 46.98745419847987, "test_em_se": 3.3390222927739184, "test_f1": 70.93337773078886, "test_f1_se": 2.844693398919915}}, "num_model_parameters": 7241732096, "max_sequence_length": 32672, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"bertscore": 0.6420851951552322, "rouge_l": 0.17051291138777958}, {"bertscore": 0.6358320617291611, "rouge_l": 0.16130353750986182}, {"bertscore": 0.6493636556406273, "rouge_l": 0.18291720508209186}, {"bertscore": 0.5799389146413887, "rouge_l": 0.11222177528194667}, {"bertscore": 0.6492291000467958, "rouge_l": 0.18622679087046023}, {"bertscore": 0.651726338910521, "rouge_l": 0.18449647815280967}, {"bertscore": 0.6445019675884396, "rouge_l": 0.17527977402720685}, {"bertscore": 0.6494749187550042, "rouge_l": 0.1791888097210878}, {"bertscore": 0.6438791200489504, "rouge_l": 0.17194362133322894}, {"bertscore": 0.604283374545048, "rouge_l": 0.12333735199297234}]}, "total": {"test_bertscore": 63.50314647061168, "test_bertscore_se": 1.474467579819928, "test_rouge_l": 16.474282553594456, "test_rouge_l_se": 1.609858592039463}}, "num_model_parameters": 7241732096, "max_sequence_length": 32896, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"mcc": 0.2931179282182586, "accuracy": 0.46875}, {"mcc": 0.3000556520027679, "accuracy": 0.47314453125}, {"mcc": 0.276802619040735, "accuracy": 0.4560546875}, {"mcc": 0.25567018679428194, "accuracy": 0.44091796875}, {"mcc": 0.2764285674250365, "accuracy": 0.45654296875}, {"mcc": 0.29026701421365375, "accuracy": 0.46630859375}, {"mcc": 0.26552546993229414, "accuracy": 0.447265625}, {"mcc": 0.2785495411942064, "accuracy": 0.46044921875}, {"mcc": 0.2653809719038501, "accuracy": 0.4462890625}, {"mcc": 0.26651592273820685, "accuracy": 0.45166015625}]}, "total": {"test_mcc": 27.683138734632912, "test_mcc_se": 0.8768770468200867, "test_accuracy": 45.673828125, "test_accuracy_se": 0.6514689483281998}}, "num_model_parameters": 7241732096, "max_sequence_length": 32643, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"accuracy": 0.3486328125, "mcc": 0.1563681927750952}, {"accuracy": 0.279296875, "mcc": 0.058321131320812306}, {"accuracy": 0.38232421875, "mcc": 0.1825301096104228}, {"accuracy": 0.38916015625, "mcc": 0.18948531711117864}, {"accuracy": 0.3271484375, "mcc": 0.110978505100635}, {"accuracy": 0.2802734375, "mcc": 0.052502305158262846}, {"accuracy": 0.34521484375, "mcc": 0.1338236395148703}, {"accuracy": 0.37451171875, "mcc": 0.17247639876554186}, {"accuracy": 0.39501953125, "mcc": 0.20470214663816644}, {"accuracy": 0.37841796875, "mcc": 0.18929996575940616}]}, "total": {"test_accuracy": 35.0, "test_accuracy_se": 2.647509711942869, "test_mcc": 14.504877117543916, "test_mcc_se": 3.404228634878846}}, "num_model_parameters": 7241732096, "max_sequence_length": 32643, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "mistralai/Mistral-7B-v0.1", "results": {"raw": {"test": [{"test_speed": 1680.84, "test_speed_short": 207.45999999999998}, {"test_speed": 2906.54, "test_speed_short": 370.4}, {"test_speed": 3144.32, "test_speed_short": 704.9}, {"test_speed": 4018.2, "test_speed_short": 863.8599999999999}, {"test_speed": 4330.16, "test_speed_short": 1025.36}, {"test_speed": 4558.22, "test_speed_short": 1326.0800000000002}, {"test_speed": 5176.08, "test_speed_short": 1532.18}, {"test_speed": 5277.82, "test_speed_short": 1664.28}, {"test_speed": 5221.16, "test_speed_short": 1835.17}, {"test_speed": 5015.12, "test_speed_short": 1992.1}]}, "total": {"test_speed": 4132.846, "test_speed_se": 747.4458980707416, "test_speed_short": 1152.1789999999999, "test_speed_short_se": 381.6146535067364}}, "num_model_parameters": 7241732096, "max_sequence_length": 32641, "vocabulary_size": 32000, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "12.10.0"}
